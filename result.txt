# of only_left/shared/only_right entities:  2953 / 127 / 663
Number of relations:  4

Using the 4 relations all together, not using the json data:

n = 1
### MICRO:
	-- left   >> mean: 624.7888, median: 422.0, hits@1: 0.137%
	-- right  >> mean: 45.14663, median: 9.0, hits@1: 16.003%
	-- global >> mean: 334.96772, median: 53.5, hits@1: 8.07%
### MACRO:
	-- left   >> mean: 681.63949, median: 453.875, hits@1: 0.132%
	-- right  >> mean: 22.38433, median: 5.75, hits@1: 30.707%
	-- global >> mean: 352.01191, median: 34.625, hits@1: 15.419%

n = 10
### MICRO:
        -- left   >> mean: 624.7888, median: 422.0, hits@10: 3.091%
        -- right  >> mean: 45.14663, median: 9.0, hits@10: 54.224%
        -- global >> mean: 334.96772, median: 53.5, hits@10: 28.657%
### MACRO:
        -- left   >> mean: 681.63949, median: 453.875, hits@10: 2.136%
        -- right  >> mean: 22.38433, median: 5.75, hits@10: 73.466%
        -- global >> mean: 352.01191, median: 34.625, hits@10: 37.801%

1. Predicting type: consume

n = 1
### MICRO:
	-- left   >> mean: 707.31636, median: 449.0, hits@1: 0.364%
	-- right  >> mean: 2.91636, median: 1.0, hits@1: 62.182%
	-- global >> mean: 355.11636, median: 17.0, hits@1: 31.273%
### MACRO:
	-- left   >> mean: 707.31636, median: 449.0, hits@1: 0.364%
	-- right  >> mean: 2.91636, median: 1.0, hits@1: 62.182%
	-- global >> mean: 355.11636, median: 17.0, hits@1: 31.273%

n = 10
### MICRO:
	-- left   >> mean: 707.31636, median: 449.0, hits@10: 1.455%
	-- right  >> mean: 2.91636, median: 1.0, hits@10: 93.818%
	-- global >> mean: 355.11636, median: 17.0, hits@10: 47.636%
### MACRO:
	-- left   >> mean: 707.31636, median: 449.0, hits@10: 1.455%
	-- right  >> mean: 2.91636, median: 1.0, hits@10: 93.818%
	-- global >> mean: 355.11636, median: 17.0, hits@10: 47.636%

2. Predicting: dosage

n = 1
### MICRO:
	-- left   >> mean: 515.60822, median: 318.0, hits@1: 0.0%
	-- right  >> mean: 5.19178, median: 2.0, hits@1: 33.699%
	-- global >> mean: 260.4, median: 19.0, hits@1: 16.849%
### MACRO:
	-- left   >> mean: 515.60822, median: 318.0, hits@1: 0.0%
	-- right  >> mean: 5.19178, median: 2.0, hits@1: 33.699%
	-- global >> mean: 260.4, median: 19.0, hits@1: 16.849%

n = 10
### MICRO:
	-- left   >> mean: 515.60822, median: 318.0, hits@10: 1.918%
	-- right  >> mean: 5.19178, median: 2.0, hits@10: 90.411%
	-- global >> mean: 260.4, median: 19.0, hits@10: 46.164%
### MACRO:
	-- left   >> mean: 515.60822, median: 318.0, hits@10: 1.918%
	-- right  >> mean: 5.19178, median: 2.0, hits@10: 90.411%
	-- global >> mean: 260.4, median: 19.0, hits@10: 46.164%

3. Predicting: ingredient

n = 1
### MICRO:
	-- left   >> mean: 551.6312, median: 385.5, hits@1: 0.165%
	-- right  >> mean: 67.45976, median: 17.0, hits@1: 3.583%
	-- global >> mean: 309.54548, median: 61.0, hits@1: 1.874%
### MACRO:
	-- left   >> mean: 551.6312, median: 385.5, hits@1: 0.165%
	-- right  >> mean: 67.45976, median: 17.0, hits@1: 3.583%
	-- global >> mean: 309.54548, median: 61.0, hits@1: 1.874%


n = 10
### MICRO:
	-- left   >> mean: 551.6312, median: 385.5, hits@10: 4.079%
	-- right  >> mean: 67.45976, median: 17.0, hits@10: 36.053%
	-- global >> mean: 309.54548, median: 61.0, hits@10: 20.066%
### MACRO:
	-- left   >> mean: 551.6312, median: 385.5, hits@10: 4.079%
	-- right  >> mean: 67.45976, median: 17.0, hits@10: 36.053%
	-- global >> mean: 309.54548, median: 61.0, hits@10: 20.066%

4. predicting: package

n = 1
### MICRO:
	-- left   >> mean: 952.00218, median: 663.0, hits@1: 0.0%
	-- right  >> mean: 13.96943, median: 3.0, hits@1: 23.362%
	-- global >> mean: 482.98581, median: 41.5, hits@1: 11.681%
### MACRO:
	-- left   >> mean: 952.00218, median: 663.0, hits@1: 0.0%
	-- right  >> mean: 13.96943, median: 3.0, hits@1: 23.362%
	-- global >> mean: 482.98581, median: 41.5, hits@1: 11.681%

n = 10
### MICRO:
	-- left   >> mean: 952.00218, median: 663.0, hits@10: 1.092%
	-- right  >> mean: 13.96943, median: 3.0, hits@10: 73.581%
	-- global >> mean: 482.98581, median: 41.5, hits@10: 37.336%
### MACRO:
	-- left   >> mean: 952.00218, median: 663.0, hits@10: 1.092%
	-- right  >> mean: 13.96943, median: 3.0, hits@10: 73.581%
	-- global >> mean: 482.98581, median: 41.5, hits@10: 37.336%


other embedding, problem of multiple entities of the same relation, 
more data, instead of simply adding, other combination, classifier to 
decide if the relation is valid, SME
similar in the embedding space have similar character level, rare things
to see the actual embedding
t-SNE to visualize
rare: 3 times instead of 5 times
information extraction system, Lidong, combine tuples
no RNN now, few more experiments, other system, combine Lidong, 




==================== April 14 ====================
with the json data in the training set, but using the old test set, 
the low accuracy is due to the increase of entities in the embedding,
choosing from so many entities become difficult.

n = 1:
### MICRO:
	-- left   >> mean: 7283.80529, median: 1488.5, hits@1: 0.069%
	-- right  >> mean: 14436.17479, median: 98.0, hits@1: 3.468%
	-- global >> mean: 10859.99004, median: 488.0, hits@1: 1.769%
### MACRO:
	-- left   >> mean: 8072.67186, median: 1617.5, hits@1: 0.028%
	-- right  >> mean: 5869.86995, median: 82.0, hits@1: 8.415%
	-- global >> mean: 6971.27091, median: 413.125, hits@1: 4.221%

n = 10:
### MICRO:
        -- left   >> mean: 7283.80529, median: 1488.5, hits@10: 0.343%
        -- right  >> mean: 14436.17479, median: 98.0, hits@10: 13.255%
        -- global >> mean: 10859.99004, median: 488.0, hits@10: 6.799%
### MACRO:
        -- left   >> mean: 8072.67186, median: 1617.5, hits@10: 0.138%
        -- right  >> mean: 5869.86995, median: 82.0, hits@10: 24.74%
        -- global >> mean: 6971.27091, median: 413.125, hits@10: 12.439%


==================== April 20 =====================
Managed to extract the embeddings of the prescription entities from the 
json embedding file, by calling the set_value() function in theano.shared
class, also we need to modify the embedding[1] and embedding[2] since 
they are used when computing scores and not just the embedding[0]. 

Another problem with the json file is that some relations appear in the 
entities, which means that they are mixed together, this will result in 
the inconsistency in the idx2entity and entity2idx files, which will have 
different length. This could be a serious problem since the mapping from 
words to index depends on these two files and the training process could 
turn out to be updating a wrong parameter. When trying to use the embeddings
with this json file, we get lower accuracy, even after we only consider
the entities appear in the prescription data.

The solution is to replace relations with a unique phrase, say we replace 
CONSUMED_IN with REL_CONSUMED_IN, which will not appear in the lhs or rhs.

result:

Using the full embedding list from the json file, including the entities 
from the json file.
n = 10:
### MICRO:
    -- left   >> mean: 1914.36882, median: 1168.0, hits@10: 0.721%
    -- right  >> mean: 2449.66003, median: 37.0, hits@10: 27.988%
    -- global >> mean: 2182.01442, median: 307.0, hits@10: 14.354%
### MACRO:
    -- left   >> mean: 1847.05422, median: 1390.25, hits@10: 0.407%
    -- right  >> mean: 991.32638, median: 29.5, hits@10: 48.398%
    -- global >> mean: 1419.1903, median: 141.875, hits@10: 24.403%

n = 1:
### MICRO:
    -- left   >> mean: 1914.36882, median: 1168.0, hits@1: 0.034%
    -- right  >> mean: 2449.66003, median: 37.0, hits@1: 7.212%
    -- global >> mean: 2182.01442, median: 307.0, hits@1: 3.623%
### MACRO:
    -- left   >> mean: 1847.05422, median: 1390.25, hits@1: 0.014%
    -- right  >> mean: 991.32638, median: 29.5, hits@1: 15.131%
    -- global >> mean: 1419.1903, median: 141.875, hits@1: 7.572%

Using the high score json triples, with only the entities from the 
prescription dataset:

